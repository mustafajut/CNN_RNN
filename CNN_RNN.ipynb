{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LELno_TGjAFF"
      },
      "source": [
        " **Implement a simple feedforward neural network and backpropagation network\n",
        " Optimize and train the neural network with various techniques. discuss it is overfit and underfit.\n",
        " Implement a CNN and RNN for a specific task and perform Fine-tune and optimize CNN and RNN using advanced techniques. **\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdMJKe8dPZpo"
      },
      "source": [
        "**Part 1: Implementing a Simple Feedforward Neural Network**\n",
        "We'll use TensorFlow/Keras for this implementation. Let's create a basic feedforward neural network for a classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q7wuGusPUX9",
        "outputId": "663ba092-3249-42cb-948a-7512579219ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.8670 - loss: 0.4635 - val_accuracy: 0.9582 - val_loss: 0.1496\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.9644 - loss: 0.1189 - val_accuracy: 0.9699 - val_loss: 0.1034\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9768 - loss: 0.0762 - val_accuracy: 0.9722 - val_loss: 0.0995\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9827 - loss: 0.0546 - val_accuracy: 0.9723 - val_loss: 0.0988\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9880 - loss: 0.0389 - val_accuracy: 0.9727 - val_loss: 0.0984\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9900 - loss: 0.0322 - val_accuracy: 0.9726 - val_loss: 0.0955\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9919 - loss: 0.0272 - val_accuracy: 0.9720 - val_loss: 0.1102\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9932 - loss: 0.0215 - val_accuracy: 0.9746 - val_loss: 0.0995\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.9942 - loss: 0.0175 - val_accuracy: 0.9753 - val_loss: 0.1040\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9951 - loss: 0.0153 - val_accuracy: 0.9708 - val_loss: 0.1300\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9673 - loss: 0.1589\n",
            "FNN Test accuracy: 0.9718\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28*28).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 28*28).astype('float32') / 255.0\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Build the FNN model\n",
        "model_fnn = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(28*28,)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_fnn.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history_fnn = model_fnn.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model_fnn.evaluate(x_test, y_test)\n",
        "print(f'FNN Test accuracy: {test_acc:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IsBmmTOQS4P"
      },
      "source": [
        "Overfitting and Underfitting in FNN\n",
        "Overfitting: If the training accuracy is much higher than the validation/test accuracy, it indicates overfitting. To mitigate this, use regularization techniques like dropout.\n",
        "Underfitting: If the training accuracy is low, the model may be too simple. Increasing the number of layers or neurons can help."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJjattkLQYsl"
      },
      "source": [
        "Part 2: Convolutional Neural Network (CNN) with MNIST Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvSbYNhfQcNn",
        "outputId": "8711fb4e-e0ef-4be7-9dce-40bd1b494ac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 71ms/step - accuracy: 0.8601 - loss: 0.4695 - val_accuracy: 0.9792 - val_loss: 0.0715\n",
            "Epoch 2/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 65ms/step - accuracy: 0.9810 - loss: 0.0615 - val_accuracy: 0.9812 - val_loss: 0.0643\n",
            "Epoch 3/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 71ms/step - accuracy: 0.9868 - loss: 0.0430 - val_accuracy: 0.9849 - val_loss: 0.0470\n",
            "Epoch 4/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 74ms/step - accuracy: 0.9895 - loss: 0.0323 - val_accuracy: 0.9886 - val_loss: 0.0408\n",
            "Epoch 5/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 66ms/step - accuracy: 0.9925 - loss: 0.0233 - val_accuracy: 0.9893 - val_loss: 0.0355\n",
            "Epoch 6/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 67ms/step - accuracy: 0.9936 - loss: 0.0183 - val_accuracy: 0.9899 - val_loss: 0.0361\n",
            "Epoch 7/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 68ms/step - accuracy: 0.9949 - loss: 0.0160 - val_accuracy: 0.9899 - val_loss: 0.0380\n",
            "Epoch 8/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 68ms/step - accuracy: 0.9956 - loss: 0.0129 - val_accuracy: 0.9894 - val_loss: 0.0391\n",
            "Epoch 9/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 69ms/step - accuracy: 0.9957 - loss: 0.0132 - val_accuracy: 0.9888 - val_loss: 0.0454\n",
            "Epoch 10/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 68ms/step - accuracy: 0.9963 - loss: 0.0110 - val_accuracy: 0.9887 - val_loss: 0.0515\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9857 - loss: 0.0514\n",
            "CNN Test accuracy: 0.9893\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Build the CNN model\n",
        "model_cnn = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_cnn.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history_cnn = model_cnn.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model_cnn.evaluate(x_test, y_test)\n",
        "print(f'CNN Test accuracy: {test_acc:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL_mC4EpQi26"
      },
      "source": [
        "Fine-Tuning and Optimizing CNN\n",
        "Data Augmentation: Apply random transformations like rotations, shifts, and zooms.\n",
        "Dropout: Add dropout layers to reduce overfitting.\n",
        "Early Stopping: Stop training when the validation loss starts increasing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VvzXLXFQq4H"
      },
      "source": [
        "Advanced Techniques for CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECIqYFKvQrjl",
        "outputId": "c8054a95-8fd6-4e90-bdf9-c9ed4e5f19ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 81ms/step - accuracy: 0.9586 - loss: 0.1421 - val_accuracy: 0.9910 - val_loss: 0.0299\n",
            "Epoch 2/50\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 82ms/step - accuracy: 0.9820 - loss: 0.0605 - val_accuracy: 0.9916 - val_loss: 0.0285\n",
            "Epoch 3/50\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 82ms/step - accuracy: 0.9856 - loss: 0.0454 - val_accuracy: 0.9917 - val_loss: 0.0255\n",
            "Epoch 4/50\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 81ms/step - accuracy: 0.9858 - loss: 0.0442 - val_accuracy: 0.9922 - val_loss: 0.0226\n",
            "Epoch 5/50\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 82ms/step - accuracy: 0.9885 - loss: 0.0362 - val_accuracy: 0.9906 - val_loss: 0.0289\n",
            "Epoch 6/50\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 82ms/step - accuracy: 0.9892 - loss: 0.0337 - val_accuracy: 0.9931 - val_loss: 0.0201\n",
            "Epoch 7/50\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 81ms/step - accuracy: 0.9896 - loss: 0.0344 - val_accuracy: 0.9934 - val_loss: 0.0231\n",
            "Epoch 8/50\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 83ms/step - accuracy: 0.9901 - loss: 0.0325 - val_accuracy: 0.9900 - val_loss: 0.0349\n",
            "Epoch 9/50\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 89ms/step - accuracy: 0.9902 - loss: 0.0293 - val_accuracy: 0.9913 - val_loss: 0.0271\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=False\n",
        ")\n",
        "\n",
        "# Define data generators\n",
        "train_generator = datagen.flow(x_train, y_train, batch_size=64)\n",
        "validation_datagen = ImageDataGenerator()  # No augmentation for validation data\n",
        "validation_generator = validation_datagen.flow(x_test, y_test, batch_size=64)\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model with augmentation and early stopping\n",
        "history_cnn = model_cnn.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THTYK0-fPhVn"
      },
      "source": [
        "**Overfitting and Underfitting**\n",
        "Overfitting: Occurs when the model performs well on the training data but poorly on unseen test data. This can be addressed by regularization techniques like dropout, weight regularization, or data augmentation.\n",
        "\n",
        "Underfitting: Happens when the model is too simple to capture the underlying patterns of the data. This can be improved by increasing the model complexity, such as adding more layers or neurons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU-pAILAQ4GV"
      },
      "source": [
        "Part 3: Recurrent Neural Network (RNN) with MNIST Data\n",
        "For RNNs, we use the MNIST data as sequences. We will reshape the data to fit the RNN input requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AwmZ0MwjQ612",
        "outputId": "a2057941-644f-49ea-9ce7-f5001bee6654"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.6667 - loss: 1.0375 - val_accuracy: 0.9153 - val_loss: 0.2866\n",
            "Epoch 2/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9090 - loss: 0.3150 - val_accuracy: 0.9273 - val_loss: 0.2430\n",
            "Epoch 3/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9298 - loss: 0.2406 - val_accuracy: 0.9367 - val_loss: 0.2153\n",
            "Epoch 4/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.9402 - loss: 0.2105 - val_accuracy: 0.9508 - val_loss: 0.1695\n",
            "Epoch 5/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.9479 - loss: 0.1771 - val_accuracy: 0.9523 - val_loss: 0.1679\n",
            "Epoch 6/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9525 - loss: 0.1672 - val_accuracy: 0.9549 - val_loss: 0.1550\n",
            "Epoch 7/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.9541 - loss: 0.1554 - val_accuracy: 0.9567 - val_loss: 0.1457\n",
            "Epoch 8/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9593 - loss: 0.1445 - val_accuracy: 0.9587 - val_loss: 0.1426\n",
            "Epoch 9/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.9612 - loss: 0.1366 - val_accuracy: 0.9600 - val_loss: 0.1435\n",
            "Epoch 10/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.9604 - loss: 0.1381 - val_accuracy: 0.9603 - val_loss: 0.1427\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9510 - loss: 0.1804\n",
            "RNN Test accuracy: 0.9579\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import SimpleRNN, Embedding\n",
        "\n",
        "# Load and preprocess data\n",
        "x_train_rnn = x_train.reshape(-1, 28, 28)  # Sequences of 28 timesteps, each of length 28\n",
        "x_test_rnn = x_test.reshape(-1, 28, 28)\n",
        "\n",
        "# Build the RNN model\n",
        "model_rnn = Sequential([\n",
        "    SimpleRNN(64, input_shape=(28, 28)),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_rnn.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history_rnn = model_rnn.fit(x_train_rnn, y_train, epochs=10, batch_size=64, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model_rnn.evaluate(x_test_rnn, y_test)\n",
        "print(f'RNN Test accuracy: {test_acc:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyPkrTH4Q_3l"
      },
      "source": [
        "Fine-Tuning and Optimizing RNN\n",
        "LSTM/GRU: Use more advanced RNN cells like LSTM or GRU to capture long-term dependencies.\n",
        "Bidirectional RNN: Process data in both forward and backward directions.\n",
        "Dropout: Apply dropout to RNN layers to prevent overfitting.\n",
        "Advanced Techniques for RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MS06ilBGRC9X",
        "outputId": "310df7ac-ff27-4de9-d98b-a82bd9886b6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 76ms/step - accuracy: 0.7014 - loss: 0.9049 - val_accuracy: 0.9463 - val_loss: 0.1734\n",
            "Epoch 2/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 76ms/step - accuracy: 0.9512 - loss: 0.1593 - val_accuracy: 0.9637 - val_loss: 0.1193\n",
            "Epoch 3/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 77ms/step - accuracy: 0.9699 - loss: 0.0970 - val_accuracy: 0.9747 - val_loss: 0.0856\n",
            "Epoch 4/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 77ms/step - accuracy: 0.9790 - loss: 0.0704 - val_accuracy: 0.9813 - val_loss: 0.0607\n",
            "Epoch 5/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 76ms/step - accuracy: 0.9831 - loss: 0.0551 - val_accuracy: 0.9774 - val_loss: 0.0737\n",
            "Epoch 6/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 77ms/step - accuracy: 0.9869 - loss: 0.0436 - val_accuracy: 0.9816 - val_loss: 0.0610\n",
            "Epoch 7/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 76ms/step - accuracy: 0.9877 - loss: 0.0405 - val_accuracy: 0.9847 - val_loss: 0.0543\n",
            "Epoch 8/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 75ms/step - accuracy: 0.9880 - loss: 0.0386 - val_accuracy: 0.9846 - val_loss: 0.0541\n",
            "Epoch 9/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 73ms/step - accuracy: 0.9898 - loss: 0.0328 - val_accuracy: 0.9858 - val_loss: 0.0520\n",
            "Epoch 10/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 74ms/step - accuracy: 0.9924 - loss: 0.0243 - val_accuracy: 0.9853 - val_loss: 0.0525\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.9821 - loss: 0.0575\n",
            "LSTM Test accuracy: 0.9869\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import LSTM, Bidirectional\n",
        "\n",
        "# Build the LSTM model\n",
        "model_lstm = Sequential([\n",
        "    Bidirectional(LSTM(64, return_sequences=True), input_shape=(28, 28)),\n",
        "    LSTM(64),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_lstm.compile(optimizer='adam',\n",
        "                   loss='categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history_lstm = model_lstm.fit(x_train_rnn, y_train, epochs=10, batch_size=64, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model_lstm.evaluate(x_test_rnn, y_test)\n",
        "print(f'LSTM Test accuracy: {test_acc:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFlHqbZcRTmq"
      },
      "source": [
        "**Feedforward Neural Network (FNN):** Best for simple tasks; monitor for overfitting and underfitting by comparing training and validation performance.\n",
        "**Convolutional Neural Network (CNN): ** Excellent for image data; use data augmentation, dropout, and early stopping to prevent overfitting.\n",
        "**Recurrent Neural Network (RNN):** Suitable for sequence data; advanced techniques like LSTM/GRU and bidirectional RNNs can improve performance."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}